The S3 object storage is not too well suited for Spark because of its architectural peculiarities like 'eventual consistency' and response time dependency on the number of objects in the bucket. To avoid timeout errors, it is recommended to always copy the source data from S3 to HDFS on the cluster before Spark invocation, and, vice versa, to copy the result back from HDFS to S3 after it has been computed.

EMR provides an utility named `s3-dist-cp`, but its usage is cumbersome because you must know the exact paths.

One Ring provides a Dist wrapper to automate handling of `s3-dist-cp` while focusing around your Task config, so you can still use Variables for source paths and generate result paths dynamically while not bothering yourself with `s3-dist-cp` command line.

One Ring Dist also can be used with other flavors of `dist-cp`, if it is required by your environment.

### Calling One Ring Dist

The syntax is similar to CLI:
```bash
java -jar ./DistWrapper/target/one-ring-dist.jar -c /path/to/tasks.ini -o /path/to/call_distcp.sh -S /path/to/dist_interface.file -d DIRECTION -x spark.meta
```

`-c`, `-x`, `-v`/`-V` switches have the same meaning for Dist as to CLI.

`-d` specifies the direction of the copying process:
* 'from' to copy the source data from S3 to HDFS,
* 'to' to copy the result back.

`-S` specifies the path to interface file with a list of HDFS paths of Task outputs generated by the CLI.

`-o` is the path where to output the script with full `s3-dist-cp` commands.

### Configuration

Dist has its own layer in tasks.ini, prefixed with `distcp.`, with a small set of keys.

`distcp.exe` specifies which executable should be used. By default, it has a value of 's3-dist-cp'.

`distcp.direction` sets which copy operations are implied to be performed. In addition to `-d` switch 'from' and 'to' there are:
 * 'both' to indicate the copy in both directions is required,
 * 'nop' (default) to suppress the copying.

Boolean `distcp.move` directs to remove files after copying. By default it is set to true.

`distcp.dir.to` and `distcp.dir.from` specify which HDFS paths are to be used to store files gathered from HDFS and for the results respectively, with the defaults of '/input' and '/output'. Subdirectories named after DataStreams will be automatically created for their files under these paths.

`distcp.store` and `distcp.ini` provide another way to set `-S` and `-o` values (but command line switches always have higher priority and override these keys if set).

### Usage

When CLI encounters `distcp.direction` directive in the config, it transparently replaces all its S3 input and output paths with HDFS paths according to the provided direction.

This is useful for multi-Process tasks.ini. If the outputs from the first Task are stored in HDFS, it allows next Tasks to consume them without a round-trip to S3 while still providing the paths pointing to S3:
```properties
spark.task1.distcp.direction=to
spark.task2.distcp.direction=nop
spark.task3.distcp.direction=from
```
...and if same Task is executed solo, it should just use bi-directional copy:
```properties
spark.task1.distcp.direction=both
```
...without further changes to path Variables and other configuration parameters.

For any DataStream that goes to `distcp.dir.from` the CLI adds a line with the resulting HDFS path to Dist interface file (under `distcp.store` / `-S` path). That allows Dist to gather them and generate commands for the `from` direction.

Actually you should never invoke Dist manually, it is a job of automation scripts to call it before and after the execution of CLI.
